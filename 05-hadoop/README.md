# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 5-1. –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ Hadoop

## –í–∞—Ä–∏–∞–Ω—Ç 5.

## –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã:
–ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –æ–¥–Ω–æ—É–∑–ª–æ–≤–æ–≥–æ
–∫–ª–∞—Å—Ç–µ—Ä–∞ Hadoop, –æ—Å–≤–æ–∏—Ç—å –±–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π
HDFS, –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É –∏ –ø—Ä–æ—Å—Ç–µ–π—à—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –Ω–∞—É—á–∏—Ç—å—Å—è
–≤—ã–≥—Ä—É–∂–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–æ –≤–Ω–µ—à–Ω–µ–π
—Å—Ä–µ–¥–µ (Jupyter Notebook / Google Colab).

## –ó–∞–¥–∞–Ω–∏–µ:

–†—ã–Ω–æ–∫ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ - –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –∏ –ø–æ–¥—Å—á–µ—Ç –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ —Ä–∞–π–æ–Ω—É (HiveQL)
—Å—Å—ã–ª–∫–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç: https://www.kaggle.com/datasets/dansbecker/melbourne-housing-snapshot

## –•–æ–¥ —Ä–∞–±–æ—Ç—ã



```python
# create_table_correct.py
import psycopg2
import pandas as pd
import os
 
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
DB_NAME = "postgres"
DB_USER = "postgres" 
DB_PASS = "123"
DB_HOST = "localhost"
DB_PORT = "5432"
 
def safe_int(value, default=None):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ int"""
    if pd.isna(value) or value == '' or value is None:
        return default
    try:
        return int(float(value))
    except (ValueError, TypeError):
        return default
 
def safe_float(value, default=None):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float"""
    if pd.isna(value) or value == '' or value is None:
        return default
    try:
        return float(value)
    except (ValueError, TypeError):
        return default
 
def safe_str(value, default=None):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É"""
    if pd.isna(value) or value == '' or value is None:
        return default
    try:
        return str(value).strip()
    except:
        return default
 
try:
    # –ß–∏—Ç–∞–µ–º CSV
    print("üìä –ß—Ç–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–∞...")
    df = pd.read_csv('melb_data.csv')
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}")
 
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ PostgreSQL
    print("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL...")
    conn = psycopg2.connect(
        database=DB_NAME,
        user=DB_USER,
        password=DB_PASS,
        host=DB_HOST,
        port=DB_PORT
    )
    conn.autocommit = True
    cursor = conn.cursor()
    print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ!")
 
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
    print("üóÉÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã...")
    cursor.execute("DROP TABLE IF EXISTS melbourne_housing")
 
    create_table_query = """
    CREATE TABLE melbourne_housing (
        id SERIAL PRIMARY KEY,
        suburb VARCHAR(100),
        address TEXT,
        rooms INTEGER,
        type VARCHAR(50),
        price DECIMAL(15,2),
        method VARCHAR(50),
        seller_g VARCHAR(100),
        date TEXT,
        distance DECIMAL(8,2),
        postcode VARCHAR(10),
        bedroom INTEGER,
        bathroom INTEGER,
        car INTEGER,
        landsize INTEGER,
        building_area DECIMAL(10,2),
        year_built INTEGER,
        council_area VARCHAR(100),
        regionname VARCHAR(100),
        property_count INTEGER
    )
    """
    cursor.execute(create_table_query)
    print("‚úÖ –¢–∞–±–ª–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
 
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ú–ò –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
    print("‚¨ÜÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    total_rows = len(df)
    success_count = 0
    error_count = 0
 
    for index, row in df.iterrows():
        if index % 2000 == 0:
            print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {index} —Å—Ç—Ä–æ–∫...")
 
        try:
            # –û–°–ù–û–í–ù–´–ï –î–ê–ù–ù–´–ï - –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            suburb = safe_str(row.get('Suburb'))
            price = safe_float(row.get('Price'))
 
            # –ï—Å–ª–∏ –Ω–µ—Ç suburb –∏–ª–∏ price - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (—ç—Ç–æ –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è)
            if not suburb or price is None:
                error_count += 1
                continue
 
            # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å NULL - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ!
            cursor.execute("""
                INSERT INTO melbourne_housing 
                (suburb, address, rooms, type, price, method, seller_g, date, 
                 distance, postcode, bedroom, bathroom, car, landsize, 
                 building_area, year_built, council_area, regionname, property_count)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                suburb,                                                   # suburb
                safe_str(row.get('Address')),                            # address
                safe_int(row.get('Rooms')),                              # rooms
                safe_str(row.get('Type')),                               # type
                price,                                                   # price
                safe_str(row.get('Method')),                             # method
                safe_str(row.get('SellerG')),                            # seller_g
                safe_str(row.get('Date')),                               # date
                safe_float(row.get('Distance')),                         # distance
                safe_str(row.get('Postcode')),                           # postcode
                safe_int(row.get('Bedroom2')),                           # bedroom
                safe_int(row.get('Bathroom')),                           # bathroom
                safe_int(row.get('Car')),                                # car
                safe_int(row.get('Landsize')),                           # landsize
                safe_float(row.get('BuildingArea')),                     # building_area
                safe_int(row.get('YearBuilt')),                          # year_built
                safe_str(row.get('CouncilArea')),                        # council_area
                safe_str(row.get('Regionname')),                         # regionname
                safe_int(row.get('Propertycount'))                       # property_count
            ))
 
            success_count += 1
 
        except Exception as e:
            error_count += 1
            if error_count <= 3:  # –ü–æ–∫–∞–∂–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –æ—à–∏–±–∫–∏
                print(f"   –û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {index}: {e}")
            continue
 
    print(f"\nüéâ –ó–ê–ì–†–£–ó–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {success_count}/{total_rows} –∑–∞–ø–∏—Å–µ–π ({success_count/total_rows*100:.1f}%)")
    print(f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ: {error_count} –∑–∞–ø–∏—Å–µ–π")
 
    # –ü—Ä–æ–≤–µ—Ä–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞–Ω–∏—è
    cursor.execute("""
        SELECT 
            COUNT(*) as total,
            COUNT(DISTINCT suburb) as unique_suburbs,
            AVG(price) as avg_price,
            MIN(price) as min_price,
            MAX(price) as max_price
        FROM melbourne_housing
    """)
    stats = cursor.fetchone()
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∑–∞–¥–∞–Ω–∏—è:")
    print(f"   –í—Å–µ–≥–æ –æ–±—ä–µ–∫—Ç–æ–≤: {stats[0]}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–∞–π–æ–Ω–æ–≤: {stats[1]}")
    print(f"   –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞: ${stats[2]:,.2f}")
    print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: ${stats[3]:,.2f}")
    print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: ${stats[4]:,.2f}")
 
    conn.close()
 
except Exception as e:
    print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    import traceback
    traceback.print_exc()
```

```python
# postgres_to_hdfs_correct.py
from pyspark.sql import SparkSession
import subprocess
 
# –°–æ–∑–¥–∞–µ–º Spark-—Å–µ—Å—Å–∏—é
spark = SparkSession.builder \
    .appName("PostgreSQL to HDFS") \
    .config("spark.jars", "/home/hadoop/housing_project/postgresql-42.6.0.jar") \
    .getOrCreate()
 
# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL
jdbc_url = "jdbc:postgresql://localhost:5432/postgres"
connection_properties = {
    "user": "postgres",
    "password": "123", 
    "driver": "org.postgresql.Driver"
}
 
print("üìä –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
 
try:
    # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
    df = spark.read \
        .jdbc(url=jdbc_url, table="melbourne_housing", properties=connection_properties)
 
    print(f"‚úÖ –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {df.count()} –∑–∞–ø–∏—Å–µ–π –∏–∑ PostgreSQL")
 
    # –ü–æ–∫–∞–∂–µ–º —Å—Ö–µ–º—É –¥–∞–Ω–Ω—ã—Ö
    print("üìã –°—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö:")
    df.printSchema()
 
    # –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    print("üîç –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
    df.select("suburb", "price", "rooms").show(10)
 
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π HDFS URI
    hdfs_uri = "hdfs://localhost:9000"
    hdfs_path = f"{hdfs_uri}/user/hadoop/melbourne_housing/data.parquet"
 
    print(f"üìç HDFS –ø—É—Ç—å: {hdfs_path}")
 
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ HDFS
    print("üìÅ –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ HDFS...")
    subprocess.run(['hdfs', 'dfs', '-mkdir', '-p', '/user/hadoop/melbourne_housing'])
 
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ HDFS
    print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ HDFS...")
    df.write \
        .mode("overwrite") \
        .parquet(hdfs_path)
 
    print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ HDFS!")
 
    # –ü—Ä–æ–≤–µ—Ä–∏–º —á–µ—Ä–µ–∑ HDFS –∫–æ–º–∞–Ω–¥—ã
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ HDFS...")
    result = subprocess.run(['hdfs', 'dfs', '-ls', '/user/hadoop/melbourne_housing/data.parquet'], 
                          capture_output=True, text=True)
    if result.returncode == 0:
        print("‚úÖ –§–∞–π–ª—ã –≤ HDFS:")
        print(result.stdout)
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å HDFS")
 
    # –ü—Ä–æ–≤–µ—Ä–∏–º —á–µ—Ä–µ–∑ Spark
    saved_df = spark.read.parquet(hdfs_path)
    print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ Spark: {saved_df.count()} –∑–∞–ø–∏—Å–µ–π")
 
    # –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("üîç –ü—Ä–∏–º–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ HDFS:")
    saved_df.select("suburb", "price", "rooms").show(10)
 
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    import traceback
    traceback.print_exc()
 
finally:
    spark.stop()
    print("üéâ –ì–æ—Ç–æ–≤–æ!")
```


